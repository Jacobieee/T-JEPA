import sys

sys.path.append('..')


import math
import re
from random import *
# from einops import rearrange, repeat
import numpy as np
import torch
import torch.nn as nn
# import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import pickle

# from model.baseline_jepa import _collate
from torch.nn.utils.rnn import pad_sequence
from config import Config
from model.graph_func import *


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

def init_weights(m):
    if isinstance(m, torch.nn.Linear):
        trunc_normal_(m.weight, std=0.02)
        if m.bias is not None:
            torch.nn.init.constant_(m.bias, 0)
    elif isinstance(m, torch.nn.LayerNorm):
        torch.nn.init.constant_(m.bias, 0)
        torch.nn.init.constant_(m.weight, 1.0)



class BaseBERT(nn.Module):
    def __init__(self, Config, input_dim, hidden_dim, num_layers=3, nheads=8,  maxlen: int = 201):
        super(BaseBERT, self).__init__()
        self.traj_embedding = nn.Linear(input_dim, Config.seq_embedding_dim)
        self.nheads = nheads

        self.pe = LearnablePositionalEncoding(Config.seq_embedding_dim, 0.1)

        self.max_length = maxlen

        structural_attn_layers = nn.TransformerEncoderLayer(Config.seq_embedding_dim, nheads, hidden_dim, 0.1)
        self.structural_attn = nn.TransformerEncoder(structural_attn_layers, num_layers)

        self.W = nn.Parameter(torch.randn(Config.seq_embedding_dim, Config.seq_embedding_dim))

        self.weights = nn.Parameter(torch.randn(9) / 9.)
        # Learnable bias for each node
        self.bias = nn.Parameter(torch.zeros(Config.seq_embedding_dim))
        torch.nn.init.normal_(self.bias, std=0.02)

    def forward(self, cell_emb, traj_o, src_padding_mask, adj):
        B, L, d = cell_emb.shape
        """
        query gcn
        """
        normalized_weights = F.softmax(self.weights, dim=0)
        normalized_weights = normalized_weights.view(1, 1, 9, 1)
        weighted_adj_cells = adj * normalized_weights
        weighted_mean = weighted_adj_cells.sum(dim=2)
        weighted_mean += self.bias
        n_emb = F.relu(weighted_mean)
        cell_emb = cell_emb + torch.matmul(n_emb, self.W)

        cell_emb = self.pe(cell_emb)
        # x = cell_emb
        cell_emb = self.structural_attn(cell_emb.permute(1, 0, 2), src_key_padding_mask=src_padding_mask)
        traj_emb = cell_emb
        mask = 1 - src_padding_mask.T.unsqueeze(-1).expand(traj_emb.shape).float()
        traj_emb = mask * traj_emb

        return traj_emb.permute(1,0,2)

    @staticmethod
    def get_padding_mask(padding_mask):
        """
        return padding mask of [batch, seq_len, seq_len],
        with a bool matrix of [seq_len, seq_len].
        """
        B = padding_mask.size(0)
        # cls_token_mask = torch.zeros((B, 1), device=padding_mask.device)
        # padding_mask = torch.cat((cls_token_mask, padding_mask), dim=1)
        L = padding_mask.size(1)
        padding_mask = padding_mask.unsqueeze(1)
        final_padding_mask = padding_mask.expand(B, L, L)

        return final_padding_mask.bool()

class PositionalEmbeddingGPS(nn.Module):
    """
    GPS location embedding combined with PE.
    """
    def __init__(self, d=128, max_len=512, lambda_symbol=100):
        super(PositionalEmbeddingGPS, self).__init__()
        self.max_len = max_len
        self.d = d  # Dimensionality for the output features
        self.lambda_symbol = lambda_symbol

    def forward(self, x):
        batch_size, sequence_length, _ = x.shape

        # Verify the sequence length matches max_len
        # if sequence_length != self.max_len:
        #     raise ValueError(f"Expected sequence length {self.max_len}, but got {sequence_length}")

        # Calculate omega_k for k values based on the sequence index
        # k_values = torch.arange(self.max_len, device=x.device) // 2
        k_values = torch.arange(self.max_len, device=x.device)
        omega_k = self.lambda_symbol / (1000 ** (2 * k_values.float() / self.d))

        # Compute sin and cos for both latitude and longitude
        lat = x[:, :, 0]  # Extract latitude values
        long = x[:, :, 1]  # Extract longitude values

        # Compute the sine and cosine values for latitude
        sin_vals_lat = torch.sin(omega_k * lat).unsqueeze(2).expand(-1, -1, self.d)
        cos_vals_lat = torch.cos(omega_k * lat).unsqueeze(2).expand(-1, -1, self.d)

        # Compute the sine and cosine values for longitude
        sin_vals_long = torch.sin(omega_k * long).unsqueeze(2).expand(-1, -1, self.d)
        cos_vals_long = torch.cos(omega_k * long).unsqueeze(2).expand(-1, -1, self.d)

        # Interleave sin and cos based on even/odd index i for both latitude and longitude
        result_lat = torch.where(k_values.unsqueeze(0).unsqueeze(2) % 2 == 0, sin_vals_lat, cos_vals_lat)
        result_long = torch.where(k_values.unsqueeze(0).unsqueeze(2) % 2 == 0, sin_vals_long, cos_vals_long)
        # print(result_lat.shape, result_long.shape)
        # Concatenate results along the last dimension to maintain the [batch, seg_len, 2*d] shape
        result = torch.cat((result_lat, result_long), dim=-1)

        # Set requires_grad to False
        result.requires_grad_(False)
        # print(result.shape)
        return result


class MultiLayerCrossAttention(nn.Module):
    def __init__(self, num_layers, embedding_dim, offset_dim, hidden_dim):
        super(MultiLayerCrossAttention, self).__init__()
        self.layers = nn.ModuleList([
            CrossAttention(embedding_dim, offset_dim, hidden_dim)
            for _ in range(num_layers)
        ])

    def forward(self, cell_embeddings, offsets, src_padding_mask=None):
        all_attention_weights = []
        for layer in self.layers:
            cell_embeddings, attention_weights = layer(cell_embeddings, offsets, src_padding_mask=src_padding_mask)
            all_attention_weights.append(attention_weights)

        return cell_embeddings, all_attention_weights[-1]


class CrossAttention(nn.Module):
    def __init__(self, embedding_dim, offset_dim, hidden_dim):
        super(CrossAttention, self).__init__()
        self.query_proj = nn.Linear(embedding_dim, hidden_dim)
        self.key_proj = nn.Linear(offset_dim, hidden_dim)
        self.value_proj = nn.Linear(offset_dim, hidden_dim)
        self.output_proj = nn.Linear(hidden_dim, embedding_dim)
        self.hidden_dim = hidden_dim

    def forward(self, cell_embeddings, offsets, src_padding_mask=None):
        queries = self.query_proj(cell_embeddings)
        keys = self.key_proj(offsets)
        values = self.value_proj(offsets)

        # Calculate attention scores
        scores = torch.matmul(queries, keys.transpose(-2, -1)) / (self.hidden_dim ** 0.5)

        if src_padding_mask is not None:
            scores = scores.masked_fill(src_padding_mask.unsqueeze(1), float('-inf'))

        weights = F.softmax(scores, dim=-1)
        attended_values = torch.matmul(weights, values)

        # Optionally project back to embedding dimension
        attended_values = self.output_proj(attended_values)

        return attended_values, weights



class LearnablePositionalEncoding(nn.Module):
    def __init__(self, emb_size: int, dropout: float, maxlen: int = 201):
        super(LearnablePositionalEncoding, self).__init__()
        # Instead of fixed position encoding, we use a learnable parameter
        self.pos_embedding = nn.Parameter(torch.zeros(maxlen, emb_size))
        # Initialize the positional encoding parameter
        nn.init.uniform_(self.pos_embedding, -0.1, 0.1)  # You can experiment with different initialization strategies

        self.dropout = nn.Dropout(dropout)

    def forward(self, token_embedding: torch.Tensor):
        # Assume token_embedding is of shape [batch_size, seq_len, emb_size]
        # We need to expand position encoding to match the batch size
        batch_size, seq_len, _ = token_embedding.size()
        # Expand the position embedding to the whole batch
        pos_embedding = self.pos_embedding[:seq_len, :].unsqueeze(0).repeat(batch_size, 1, 1)
        # Add the learnable position encoding to the input token embeddings
        return self.dropout(token_embedding + pos_embedding)


class PositionalEncoding(nn.Module):
    def __init__(self, emb_size: int, dropout: float, maxlen: int = 201):
        super(PositionalEncoding, self).__init__()
        den = torch.exp(torch.arange(0, emb_size, 2) * (-math.log(10000)) / emb_size)
        pos = torch.arange(0, maxlen).reshape(maxlen, 1)
        pos_embedding = torch.zeros((maxlen, emb_size))
        pos_embedding[:, 0::2] = torch.sin(pos * den)
        pos_embedding[:, 1::2] = torch.cos(pos * den)
        pos_embedding = pos_embedding.unsqueeze(0)

        self.dropout = nn.Dropout(dropout)
        self.register_buffer('pos_embedding', pos_embedding)

    def forward(self, token_embedding: torch.Tensor):
        return self.dropout(token_embedding + self.pos_embedding[:, token_embedding.size(1), :])


class PoswiseFeedForwardNet(nn.Module):
    def __init__(self, d_model, d_ff, dropout):
        super(PoswiseFeedForwardNet, self).__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.feed_forward = nn.Sequential(
            self.fc1,
            nn.GELU(),
            self.fc2
        )

    def forward(self, x):
        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)
        norm1_out = self.dropout1(self.norm1(x))

        feed_fwd_out = self.feed_forward(norm1_out)
        feed_fwd_residual_out = feed_fwd_out + norm1_out
        norm2_out = self.dropout2(self.norm2(feed_fwd_residual_out))
        return norm2_out


class EncoderLayer(nn.Module):
    def __init__(self, d_model, d_ff, d_k, d_v, n_heads, dropout):
        super(EncoderLayer, self).__init__()
        self.enc_self_attn = MultiHeadAttention(d_model, d_k, d_v, n_heads)
        self.pos_ffn = PoswiseFeedForwardNet(d_model, d_ff, dropout)

    def forward(self, enc_inputs, enc_self_attn_mask):
        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs,
                                               enc_self_attn_mask)  # enc_inputs to same Q,K,V
        enc_outputs = self.pos_ffn(enc_outputs)  # enc_outputs: [batch_size x len_q x d_model]
        return enc_outputs, attn


def gelu(x):
    "Implementation of the gelu activation function by Hugging Face"
    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))


class ScaledDotProductAttention(nn.Module):
    def __init__(self, d_k):
        super(ScaledDotProductAttention, self).__init__()
        self.d_k = d_k

    def forward(self, Q, K, V, attn_mask):
        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(
            self.d_k)  # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]
        # print(attn_mask.shape)
        # print(scores.shape)
        scores.masked_fill_(attn_mask, -1e9)  # Fills elements of self tensor with value where mask is one.
        attn = nn.Softmax(dim=-1)(scores)
        context = torch.matmul(attn, V)
        return context, attn


class MultiHeadAttention(nn.Module):
    def __init__(self, d_model=768, d_k=64, d_v=64, n_heads=12):
        super(MultiHeadAttention, self).__init__()
        self.W_Q = nn.Linear(d_model, d_k * n_heads)
        self.W_K = nn.Linear(d_model, d_k * n_heads)
        self.W_V = nn.Linear(d_model, d_v * n_heads)
        self.n_heads = n_heads
        self.d_k = d_k
        self.d_v = d_v
        self.d_model = d_model
        self.linear = nn.Linear(self.n_heads * self.d_v, self.d_model)
        self.norm = nn.LayerNorm(d_model)

    def forward(self, Q, K, V, attn_mask):
        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]
        residual, batch_size = Q, Q.size(0)
        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)
        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,
                                                                       2)  # q_s: [batch_size x n_heads x len_q x d_k]
        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,
                                                                       2)  # k_s: [batch_size x n_heads x len_k x d_k]
        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1,
                                                                       2)  # v_s: [batch_size x n_heads x len_k x d_v]

        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1,
                                                  1)  # attn_mask : [batch_size x n_heads x len_q x len_k]

        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]
        context, attn = ScaledDotProductAttention(self.d_k)(q_s, k_s, v_s, attn_mask)
        context = context.transpose(1, 2).contiguous().view(batch_size, -1,
                                                            self.n_heads * self.d_v)  # context: [batch_size x len_q x n_heads * d_v]
        output = self.linear(context)

        return self.norm(output + residual), attn  # output: [batch_size x len_q x d_model]


def generate_random_points(cell_space, num_points_per_cell=20):
    all_points = {}
    for i_x in range(cell_space.x_size):
        for i_y in range(cell_space.y_size):
            # Retrieve the boundaries of the current cell
            x_min, y_min, x_max, y_max = cell_space.get_mbr(i_x, i_y)

            # Generate points within these boundaries
            points = [(random.uniform(0, 100), random.uniform(0, 100))
                      for _ in range(num_points_per_cell)]

            # Store points using the cell ID as key
            cell_id = cell_space.get_cellid_by_xyidx(i_x, i_y)
            all_points[cell_id] = points

    return all_points

